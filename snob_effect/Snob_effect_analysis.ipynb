{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d08171bf-68ea-424b-840c-33fd95cc7a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# import feature_extract\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import torch\n",
    "import data_retrieval.psql_methods as psql\n",
    "import pickle\n",
    "import data_retrieval.alchemy_methods as alc\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from more_itertools import chunked\n",
    "import pandas as pd\n",
    "import snob_utils as snob\n",
    "from scipy.stats import pearsonr, spearmanr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54933d96-b57c-4697-9270-3d5d98331681",
   "metadata": {},
   "source": [
    "The goal of this notebook is to evaluate how effective aesthetic dissimilarity is at predicting sale value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b8522a7-ba33-47c5-950b-97e86a091148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data\n",
      "Loaded Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1184/1184 [00:04<00:00, 249.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data\n",
      "Loaded Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1265/1265 [00:03<00:00, 318.99it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Contract</th>\n",
       "      <th>NFT_num</th>\n",
       "      <th>rarity_rank</th>\n",
       "      <th>Label</th>\n",
       "      <th>Collection</th>\n",
       "      <th>sale_price</th>\n",
       "      <th>distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x69be2c18a3c7acd14d5114a66f2dad3e96150427</td>\n",
       "      <td>102</td>\n",
       "      <td>454.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0-project</td>\n",
       "      <td>0.096525</td>\n",
       "      <td>0.490676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x69be2c18a3c7acd14d5114a66f2dad3e96150427</td>\n",
       "      <td>103</td>\n",
       "      <td>496.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0-project</td>\n",
       "      <td>0.038415</td>\n",
       "      <td>0.368442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x69be2c18a3c7acd14d5114a66f2dad3e96150427</td>\n",
       "      <td>104</td>\n",
       "      <td>218.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0-project</td>\n",
       "      <td>0.034688</td>\n",
       "      <td>0.512769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x69be2c18a3c7acd14d5114a66f2dad3e96150427</td>\n",
       "      <td>106</td>\n",
       "      <td>650.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0-project</td>\n",
       "      <td>0.022950</td>\n",
       "      <td>0.542099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x69be2c18a3c7acd14d5114a66f2dad3e96150427</td>\n",
       "      <td>108</td>\n",
       "      <td>865.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0-project</td>\n",
       "      <td>0.035188</td>\n",
       "      <td>0.482177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468705</th>\n",
       "      <td>0x9c80777cae192e5031c38a0d951c55730ecc3f5e</td>\n",
       "      <td>6614</td>\n",
       "      <td>2888.0</td>\n",
       "      <td>3428</td>\n",
       "      <td>zombieclub-token</td>\n",
       "      <td>1.534742</td>\n",
       "      <td>0.612652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468706</th>\n",
       "      <td>0x9c80777cae192e5031c38a0d951c55730ecc3f5e</td>\n",
       "      <td>6628</td>\n",
       "      <td>18.0</td>\n",
       "      <td>3428</td>\n",
       "      <td>zombieclub-token</td>\n",
       "      <td>2.425428</td>\n",
       "      <td>0.712623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468707</th>\n",
       "      <td>0x9c80777cae192e5031c38a0d951c55730ecc3f5e</td>\n",
       "      <td>6640</td>\n",
       "      <td>1147.0</td>\n",
       "      <td>3428</td>\n",
       "      <td>zombieclub-token</td>\n",
       "      <td>1.253140</td>\n",
       "      <td>0.687746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468708</th>\n",
       "      <td>0x9c80777cae192e5031c38a0d951c55730ecc3f5e</td>\n",
       "      <td>969</td>\n",
       "      <td>4998.0</td>\n",
       "      <td>3428</td>\n",
       "      <td>zombieclub-token</td>\n",
       "      <td>2.493558</td>\n",
       "      <td>0.618565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468709</th>\n",
       "      <td>0x9c80777cae192e5031c38a0d951c55730ecc3f5e</td>\n",
       "      <td>986</td>\n",
       "      <td>220.0</td>\n",
       "      <td>3428</td>\n",
       "      <td>zombieclub-token</td>\n",
       "      <td>1.648898</td>\n",
       "      <td>0.762690</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>468710 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Contract NFT_num  rarity_rank  \\\n",
       "0       0x69be2c18a3c7acd14d5114a66f2dad3e96150427     102        454.0   \n",
       "1       0x69be2c18a3c7acd14d5114a66f2dad3e96150427     103        496.0   \n",
       "2       0x69be2c18a3c7acd14d5114a66f2dad3e96150427     104        218.0   \n",
       "3       0x69be2c18a3c7acd14d5114a66f2dad3e96150427     106        650.0   \n",
       "4       0x69be2c18a3c7acd14d5114a66f2dad3e96150427     108        865.0   \n",
       "...                                            ...     ...          ...   \n",
       "468705  0x9c80777cae192e5031c38a0d951c55730ecc3f5e    6614       2888.0   \n",
       "468706  0x9c80777cae192e5031c38a0d951c55730ecc3f5e    6628         18.0   \n",
       "468707  0x9c80777cae192e5031c38a0d951c55730ecc3f5e    6640       1147.0   \n",
       "468708  0x9c80777cae192e5031c38a0d951c55730ecc3f5e     969       4998.0   \n",
       "468709  0x9c80777cae192e5031c38a0d951c55730ecc3f5e     986        220.0   \n",
       "\n",
       "        Label        Collection  sale_price  distance  \n",
       "0           0         0-project    0.096525  0.490676  \n",
       "1           0         0-project    0.038415  0.368442  \n",
       "2           0         0-project    0.034688  0.512769  \n",
       "3           0         0-project    0.022950  0.542099  \n",
       "4           0         0-project    0.035188  0.482177  \n",
       "...       ...               ...         ...       ...  \n",
       "468705   3428  zombieclub-token    1.534742  0.612652  \n",
       "468706   3428  zombieclub-token    2.425428  0.712623  \n",
       "468707   3428  zombieclub-token    1.253140  0.687746  \n",
       "468708   3428  zombieclub-token    2.493558  0.618565  \n",
       "468709   3428  zombieclub-token    1.648898  0.762690  \n",
       "\n",
       "[468710 rows x 7 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_df = snob.create_master_snob_df()\n",
    "total_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f7b075f",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df.to_pickle('/global/scratch/tlundy/NFT_Research/nft_research/snob_effect/dataframes/total_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60b4114b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before torch load\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/scratch/tlundy/NFT_Research/nft_research/snob_effect/snob_utils.py:45: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  features = torch.load(feature_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after torch load\n",
      "Dataset ReturnIndexDataset\n",
      "    Number of datapoints: 846346\n",
      "    Root location: /global/scratch/tlundy/NFT_Research/nft_research/Dino/images_features/images/val\n",
      "before psql\n",
      "after psql\n",
      "after feature list\n",
      "after dict\n"
     ]
    }
   ],
   "source": [
    "df1 = snob.save_data_and_features('images','features',True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "86e04f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/scratch/tlundy/NFT_Research/nft_venv2/lib/python3.12/site-packages/scipy/stats/_stats_py.py:4781: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(stats.ConstantInputWarning(msg))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-0.027435901741978037, 3.58153435894283e-44)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_df = total_df.groupby('Collection').filter(lambda group: len(group) >= 100)\n",
    "#Keep only rows with rarity ranks\n",
    "rares = total_df.dropna(subset=['rarity_rank'],inplace=False).groupby('Collection').filter(lambda group: len(group) >= 100)\n",
    "slug_list = snob.compute_labels_pearson(rares.groupby('Collection'),'rarity_rank',metric='pearson',slug_list = True)\n",
    "result_df = snob.compute_global_corrs(total_df,norm_type='additive',slug_list=slug_list)\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5891875c",
   "metadata": {},
   "outputs": [],
   "source": [
    "snob.compare_r2(total_df,return_slugs=True)\n",
    "snob.compare_spearman(total_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9538eded-5afa-49ad-aced-dcee101bbe6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Suppose you have your data in arrays:\n",
    "# distance, price, rarity (each is an array of the same length)\n",
    "total_df = total_df.copy()\n",
    "total_df = total_df.groupby('Collection').filter(lambda group: len(group) >= 100)\n",
    "#Keep only rows with rarity ranks\n",
    "df_filtered_rare = total_df.dropna(subset=['rarity_rank'],inplace=False).groupby('Collection').filter(lambda group: len(group) >= 100)\n",
    "grouped = df_filtered_rare.groupby('Collection')\n",
    "pos_count = 0\n",
    "neg_count = 0\n",
    "for label,group in grouped:\n",
    "    rarity = group['rarity_rank']\n",
    "    distance = group['distance']\n",
    "    price = group['sale_price']\n",
    "    # 1. Regress distance on rarity\n",
    "    X = sm.add_constant(rarity)           # add intercept\n",
    "    model_dist = sm.OLS(distance, X).fit()\n",
    "    resid_distance = model_dist.resid     # these are the residuals of distance ~ rarity\n",
    "\n",
    "    # 2. Regress price on rarity\n",
    "    model_price = sm.OLS(price, X).fit()\n",
    "    resid_price = model_price.resid       # these are the residuals of price ~ rarity\n",
    "\n",
    " \n",
    "    # 3. Correlate these residuals and get p-value\n",
    "    r_partial, p_value = pearsonr(resid_distance, resid_price)\n",
    "    if p_value < 0.05 and r_partial > 0:\n",
    "        pos_count+=1\n",
    "        # print(f\"Collection: {label}\")\n",
    "        # print(f\"Partial correlation (distance, price | rarity): {r_partial:.4f}\")\n",
    "        # print(f\"p-value: {p_value:.4g}\")\n",
    "    if p_value < 0.05 and r_partial < 0:\n",
    "        neg_count+=1\n",
    "print(pos_count,neg_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea85f3ac-3a20-42c4-b04c-4854369c74c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "total_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7d82576",
   "metadata": {},
   "outputs": [],
   "source": [
    "command = \"Select contract,token_id,rank from nft_to_rarity_2\"\n",
    "rarities = psql.execute_commands([command])\n",
    "# print(rarities)\n",
    "# Column names for the DataFrame\n",
    "columns = ['Contract', 'NFT_num', 'rarity_rank']\n",
    "# Create a DataFrame from the list of tuples\n",
    "df_rare = pd.DataFrame(rarities, columns=columns)\n",
    "merged_df_rare = pd.merge(df_rare, total_df, on=['Contract','NFT_num'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abc8ab6-22e5-4401-92d8-7271848b10e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(total_df, df_rare, on=columns_to_compare, how='outer', indicator=True)\n",
    "\n",
    "# Filter rows that are only in df1\n",
    "coll_to_rare = merged_df[merged_df['_merge'] == 'left_only'].drop('_merge', axis=1)\n",
    "coll_to_rare = coll_to_rare.groupby('Collection').filter(lambda group: len(group) >= 100)\n",
    "coll_to_rare['Collection'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad70837-9fd0-4c0d-ae61-7c8914af04fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import statistics\n",
    "total_df = total_df.groupby('Collection').filter(lambda group: len(group) >= 100)\n",
    "rares = total_df.dropna(subset=['rarity_rank']).groupby('Collection').filter(lambda group: len(group) >= 100)\n",
    "grouped = rares.groupby('Collection')\n",
    "# grouped = coll_to_rare.groupby('Collection')\n",
    "# Create a scatter plot using Matplotlib\n",
    "pos_count = 0\n",
    "neg_count = 0\n",
    "total_count = 0\n",
    "\n",
    "\n",
    "correlations = []\n",
    "for label, group in tqdm(grouped):\n",
    "    if label in rows:\n",
    "        total_count+=1\n",
    "        x = group['rarity_rank']\n",
    "        y = group['sale_price']\n",
    "        def statistic(x):  # explore all possible pairings by permuting `x`\n",
    "            rs = stats.spearmanr(x, y).statistic  # ignore pvalue\n",
    "            transformed = rs * np.sqrt(dof / ((rs+1.0)*(1.0-rs)))\n",
    "            return transformed\n",
    "        # try:\n",
    "    \n",
    "        # except:\n",
    "        #     continue\n",
    "        # plt.show()\n",
    "        # Compute correlation coefficient and p-value\n",
    "        correlation, p_value = pearsonr(x, y)\n",
    "        \n",
    "        # correlation, p_value = spearmanr(x,y)\n",
    "        # ref = stats.permutation_test((x,), statistic, alternative='greater',permutation_type='pairings')\n",
    "        if p_value<0.05 and correlation>0:\n",
    "            correlations.append(correlation)\n",
    "            plt.scatter(x,y,alpha=0.2)\n",
    "            coefficients = np.polyfit(x, y, 1)\n",
    "            line_of_best_fit = np.polyval(coefficients, x)\n",
    "            # Plot the line of best fit\n",
    "            plt.plot(x, line_of_best_fit, color='red', label='Line of Best Fit')\n",
    "            plt.title('Scatter Plot')\n",
    "            plt.xlabel('Column 1')\n",
    "            plt.xlim(left=0,right=1)\n",
    "            plt.ylim(top=1,bottom=0)\n",
    "            plt.ylabel('Column 2')\n",
    "            # plt.show()\n",
    "            pos_count+=1\n",
    "            print(f\"Collection: {label}, Correlation: {correlation}, P-Value:{p_value}\")\n",
    "        elif p_value<0.05 and correlation<0:\n",
    "            neg_count+=1\n",
    "print(total_count)\n",
    "print(pos_count)\n",
    "print(neg_count)\n",
    "print(np.percentile(correlations,[5,25,50,75,95]))\n",
    "print(np.mean(correlations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290f08d4-bd5a-4ed0-8dae-b159b6aa8f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "pfp_rows = df1.query(f\"Collection in {rows}\")\n",
    "selected_columns = pfp_rows[['Contract', 'NFT_num']].to_numpy().tolist()\n",
    "# rares_to_grab  = list(chunked(selected_columns,10_000))\n",
    "# for rare_chunk in rares_to_grab:\n",
    "#     rarities = alc.NFT_to_rarities(rare_chunk)\n",
    "#     command = \"INSERT INTO nft_to_rare (contract, token_id, rarity) VALUES (%s, %s, %s) returning token_id\"\n",
    "#     psql.batch_insert(command,rarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8b3c283-0106-4ce2-9724-c6706b10f6bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39978e6-ed85-4614-987a-cc08e75d8a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_rare.query(\"Collection=='0n1-force'\").sort_values('distance')\n",
    "# merged_df_rare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9400f6-77ae-4c47-964e-32af4c252bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered_rare = merged_df_rare.groupby('Collection').filter(lambda group: len(group) >= 100)\n",
    "# Count occurrences of each unique label in the filtered DataFrame\n",
    "label_counts = df_filtered_rare['Collection'].value_counts()\n",
    "\n",
    "# Display the result\n",
    "print(\"Label counts:\")\n",
    "print(label_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97191a8e-b37a-49a9-b336-9e16c7610567",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr\n",
    "grouped = df_filtered_rare.groupby('Collection')\n",
    "# Create a scatter plot using Matplotlib\n",
    "count=0\n",
    "count_dist = 0\n",
    "count_rare = 0\n",
    "count_dist_neg = 0\n",
    "count_rare_neg = 0\n",
    "for label, group in tqdm(grouped):\n",
    "    if label in dist_slugs:\n",
    "        x_1 = group['rarity_rank']\n",
    "        x_2 = group['distance']\n",
    "        y = group['sale_price']\n",
    "        dof = dof = len(x_1)-2\n",
    "        def statistic(x):  # explore all possible pairings by permuting `x`\n",
    "            rs = stats.spearmanr(x, y).statistic  # ignore pvalue\n",
    "            transformed = rs * np.sqrt(dof / ((rs+1.0)*(1.0-rs)))\n",
    "            return transformed\n",
    "        count+=1\n",
    "        # Compute correlation coefficient and p-value\n",
    "        # correlation, p_value = pearsonr(x_1, y)\n",
    "        # correlation_2, p_value_2 = pearsonr(x_2, y)\n",
    "        ref = stats.permutation_test((x_1,), statistic, alternative='less',permutation_type='pairings')\n",
    "        correlation, p_value = spearmanr(x_1, y)\n",
    "        correlation_2, p_value_2 = spearmanr(x_2, y)\n",
    "        p_value = stats.permutation_test((x_1,), statistic, alternative='less',permutation_type='pairings').pvalue\n",
    "        p_value_2 = stats.permutation_test((x_2,), statistic, alternative='greater',permutation_type='pairings').pvalue\n",
    "        print(p_value)\n",
    "        print(p_value_2)\n",
    "        if np.abs(correlation)>np.abs(correlation_2) and p_value<0.05:\n",
    "        # if p_value<0.05:\n",
    "            if correlation<-0:\n",
    "                print(f\"(Rarity) Collection: {label}, Correlation: {correlation}, P-Value:{p_value}\")\n",
    "                count_rare_neg+=1\n",
    "            else:\n",
    "                print(f\"(Rarity) Collection: {label}, Correlation: {correlation}, P-Value:{p_value}\")\n",
    "                count_rare+=1\n",
    "        elif p_value_2<0.05:\n",
    "            if correlation_2<0:\n",
    "                count_dist_neg+=1\n",
    "            else:\n",
    "                count_dist+=1\n",
    "print(count)\n",
    "print(f\"Rare: {count_rare}, Rare Neg: {count_rare_neg}, Dist: {count_dist}, Dist Neg:{count_dist_neg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59ef12b-5236-48a7-9b76-e554deb5e94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "from sklearn import datasets, linear_model\n",
    "grouped = df_filtered_rare.groupby('Collection')\n",
    "# Create a scatter plot using Matplotlib\n",
    "count=0\n",
    "count_dist = 0\n",
    "count_rare = 0\n",
    "count_dist_neg = 0\n",
    "count_rare_neg = 0\n",
    "rare_r2s = []\n",
    "dist_r2s = []\n",
    "dist_slugs = []\n",
    "for label, group in tqdm(grouped):\n",
    "    # if label not in rows:\n",
    "    x_1 = -np.asarray(group['rarity_rank'])\n",
    "    x_1 = x_1.reshape(-1, 1)\n",
    "    x_2 = np.asarray(group['distance'])\n",
    "    x_2 = x_2.reshape(-1, 1)\n",
    "    y = np.asarray(group['sale_price'])\n",
    "    y = y.reshape(-1, 1)\n",
    "    # y = np.log10(y, out=np.zeros_like(y), where=(y!=0))\n",
    "    # Create linear regression object for rarity\n",
    "    regr1 = linear_model.LinearRegression(positive=True)\n",
    "    regr1.fit(x_1, y)\n",
    "    rarity_pred = regr1.predict(x_1)\n",
    "    # Create linear regression object for rarity\n",
    "    regr2 = linear_model.LinearRegression(positive=True)\n",
    "    regr2.fit(x_2, y)\n",
    "    visual_pred = regr2.predict(x_2)\n",
    "    # Compute correlation coefficient and p-value\n",
    "    rare_r2 = r2_score(y,rarity_pred)\n",
    "    # rare_r2s.append(rare_r2)\n",
    "    dist_r2= r2_score(y,visual_pred)\n",
    "    # dist_r2s.append(dist_r2)\n",
    "    if max(rare_r2,dist_r2) ==0:\n",
    "        continue\n",
    "    if rare_r2>dist_r2:\n",
    "        count_rare+=1\n",
    "        rare_r2s.append(rare_r2)\n",
    "    else:\n",
    "        print(f\"(Visual) Collection: {label}, R2: {dist_r2}\")\n",
    "        count_dist+=1\n",
    "        dist_slugs.append(label)\n",
    "        dist_r2s.append(dist_r2)\n",
    "print(count_rare,count_dist,np.mean(rare_r2s),np.mean(dist_r2s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f4743b-a6ed-45b4-b0f8-0dbd45b59821",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "from sklearn import datasets, linear_model\n",
    "grouped = df_filtered_rare.groupby('Collection')\n",
    "# Create a scatter plot using Matplotlib\n",
    "count=0\n",
    "count_dist = 0\n",
    "count_rare = 0\n",
    "count_dist_neg = 0\n",
    "count_rare_neg = 0\n",
    "rare_r2s = []\n",
    "dist_r2s = []\n",
    "for label, group in tqdm(grouped):\n",
    "    x_1 = -np.asarray(group['rarity_rank'])\n",
    "    x_1 = x_1.reshape(-1, 1)\n",
    "    x_2 = np.asarray(group['distance'])\n",
    "    x_2 = x_2.reshape(-1, 1)\n",
    "    y = np.asarray(group['sale_price'])\n",
    "    y = y.reshape(-1, 1)\n",
    "    combined_array = np.hstack((x_1, x_2))\n",
    "    # y = np.log10(y, out=np.zeros_like(y), where=(y!=0))\n",
    "    # Create linear regression object for rarity\n",
    "    regr1 = linear_model.LinearRegression()\n",
    "    regr1.fit(combined_array, y)\n",
    "    rarity_pred = regr1.predict(combined_array)\n",
    "    # Create linear regression object for rarity\n",
    "    # Compute correlation coefficient and p-value\n",
    "    rare_r2 = r2_score(y,rarity_pred)\n",
    "    rare_r2s.append(rare_r2)\n",
    "    # dist_r2= r2_score(y,visual_pred)\n",
    "    # dist_r2s.append(dist_r2)\n",
    "    if rare_r2>0.1:\n",
    "        count_rare+=1\n",
    "    # elif regr2.coef_>0 and dist_r2>0.2:\n",
    "    #     count_dist+=1\n",
    "print(count_rare,np.median(rare_r2s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a650d6-00c3-4098-a00b-3d21ccc98fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('rarity_distance.pkl','wb') as f:\n",
    "    pickle.dump(df_filtered_rare,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74016bc5-dc54-42c6-8340-c422fd41f7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('rarity_distance.pkl','rb') as f:\n",
    "    df_filtered_rare = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4275a151-251c-49a6-b568-069e7995185f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Count occurrences of each unique label in the filtered DataFrame\n",
    "label_counts = df_filtered_rare['rarity'].value_counts()\n",
    "\n",
    "# Display the result\n",
    "print(\"Label counts:\")\n",
    "print(label_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3af29b-23aa-4a00-8986-15343e74c88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered_rare.query(\"Collection=='sakura-park'\").sort_values('rarity_rank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9d5ac7-f57c-4971-a706-be8c42455e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr\n",
    "grouped = df_filtered_rare.groupby('Collection')\n",
    "# Create a scatter plot using Matplotlib\n",
    "count=0\n",
    "count_dist = 0\n",
    "count_rare = 0\n",
    "count_dist_neg = 0\n",
    "count_rare_neg = 0\n",
    "for label, group in tqdm(grouped):\n",
    "    x_1 = 1-group['rarity']\n",
    "    y = group['sale_price']\n",
    "    # Compute correlation coefficient and p-value\n",
    "    correlation, p_value = pearsonr(x_1, y)\n",
    "    if p_value<0.05:\n",
    "        x_1 = np.log(x_1)\n",
    "        plt.scatter(x_1,y,alpha=0.2,label='Rarity')\n",
    "        plt.title('Scatter Plot')\n",
    "        plt.xlabel('Column 1')\n",
    "        # plt.xlim(left=0,right=1)\n",
    "        plt.ylabel('Column 2')\n",
    "        try:\n",
    "            coefficients = np.polyfit(x_1, y, 1)\n",
    "            line_of_best_fit = np.polyval(coefficients, x_1)\n",
    "            # Plot the line of best fit\n",
    "            plt.plot(x_1, line_of_best_fit, color='red', label='Line of Best Fit')\n",
    "        except:\n",
    "            continue\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "        count+=1\n",
    "        print(f\"(Rarity) Collection: {label}, Correlation: {correlation}, P-Value:{p_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07e26b3-1db7-47b2-9ff4-3d0da95ed5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "beans_df = merged_df2.query(\"Collection=='beanzofficial'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2895be28-ae22-4d36-bbb6-ac773bceed60",
   "metadata": {},
   "outputs": [],
   "source": [
    "beans_df.sort_values(by=\"distance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c40ef1-8466-406d-a195-9024d775d637",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Openseas_Methods as opse\n",
    "shit_list = []\n",
    "for contract,token in tqdm(selected_columns[:10]):\n",
    "    if contract in shit_list:\n",
    "        continue\n",
    "    rarity = opse.pull_nft_rarity(contract,token)\n",
    "    if rarity is None:\n",
    "        shit_list.append(contract)\n",
    "    else:\n",
    "        print(rarity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63234367-139a-4069-b191-1d0a24bd8967",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_compare = ['Contract', 'NFT_num']\n",
    "\n",
    "# Merge DataFrames based on specified columns\n",
    "merged_df = pd.merge(total_df, df_rare, on=columns_to_compare, how='outer', indicator=True)\n",
    "\n",
    "# Filter rows that are only in df1\n",
    "coll_to_rare = merged_df[merged_df['_merge'] == 'left_only'].drop('_merge', axis=1)\n",
    "coll_to_rare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496ebdbe-7e09-4d63-8436-da54caccb729",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = sorted(coll_to_rare[['Contract', 'NFT_num']].to_numpy().tolist())\n",
    "command = \"INSERT INTO nft_to_rarity_2 (contract, token_id, rare_score, rank) VALUES (%s, %s, %s,%s) returning token_id\"\n",
    "rares_to_grab  = list(chunked(selected_columns,5000))\n",
    "rarities = []\n",
    "print(len(rares_to_grab))\n",
    "bad_contracts = []\n",
    "for rare_chunk in rares_to_grab[1:]:\n",
    "    for contract,token_id in tqdm(rare_chunk):\n",
    "        if contract in bad_contracts:\n",
    "            continue\n",
    "        rare_dict = opse.pull_nft_rarity(contract,token_id)\n",
    "        if rare_dict is None:\n",
    "            bad_contracts.append(contract)\n",
    "            continue\n",
    "        rarities.append((contract,token_id,rare_dict.get('score',''),rare_dict.get('rank',-1)))\n",
    "    psql.batch_insert(command,rarities)\n",
    "    rarities = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fdc3bc-cc72-4007-8223-19523e26ff5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('poo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefbb6a0-bad4-4395-9870-6854f62cf14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from scipy import stats\n",
    "import statistics\n",
    "grouped = total_df.groupby('Collection')\n",
    "# grouped = coll_to_rare.groupby('Collection')\n",
    "# Create a scatter plot using Matplotlib\n",
    "pos_count = 0\n",
    "neg_count = 0\n",
    "total_count = 0\n",
    "\n",
    "corr_map = {}\n",
    "correlations = []\n",
    "for label, group in tqdm(grouped):\n",
    "    if label not in rows:\n",
    "        total_count+=1\n",
    "        x = group['rarity_rank']\n",
    "        y = group['sale_price']\n",
    "        def statistic(x):  # explore all possible pairings by permuting `x`\n",
    "            rs = stats.spearmanr(x, y).statistic  # ignore pvalue\n",
    "            transformed = rs * np.sqrt(dof / ((rs+1.0)*(1.0-rs)))\n",
    "            return transformed\n",
    "        # try:\n",
    "\n",
    "        # except:\n",
    "        #     continue\n",
    "        # plt.show()\n",
    "        # Compute correlation coefficient and p-value\n",
    "        correlation, p_value = pearsonr(x, y)\n",
    "        corr_map[label] = (correlation,p_value)\n",
    "        # correlation, p_value = spearmanr(x,y)\n",
    "        # ref = stats.permutation_test((x,), statistic, alternative='greater',permutation_type='pairings')\n",
    "        if p_value<0.05 and correlation>0:\n",
    "            correlations.append(correlation)\n",
    "            plt.scatter(x,y,alpha=0.2)\n",
    "            coefficients = np.polyfit(x, y, 1)\n",
    "            line_of_best_fit = np.polyval(coefficients, x)\n",
    "            # Plot the line of best fit\n",
    "            plt.plot(x, line_of_best_fit, color='red', label='Line of Best Fit')\n",
    "            plt.title('Scatter Plot')\n",
    "            plt.xlabel('Column 1')\n",
    "            plt.xlim(left=0,right=1)\n",
    "            plt.ylim(top=1,bottom=0)\n",
    "            plt.ylabel('Column 2')\n",
    "            # plt.show()\n",
    "            pos_count+=1\n",
    "            # print(f\"Collection: {label}, Correlation: {correlation}, P-Value:{p_value}\")\n",
    "        elif p_value<0.05 and correlation<0:\n",
    "            neg_count+=1\n",
    "print(total_count)\n",
    "print(pos_count)\n",
    "print(neg_count)\n",
    "print(np.percentile(correlations,[5,25,50,75,95]))\n",
    "print(np.mean(correlations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3ba6b072-f03c-468f-ac5c-bcaaccf902e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_rareq = merged_df_rare.query(\"Collection=='invazers'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7264c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot for distance_quantile and sale_price_quantile\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(merged_df_rareq['distance_quantile'], merged_df_rareq['sale_price_quantile'],alpha=0.1)\n",
    "plt.xlabel('Distance Quantile')\n",
    "plt.ylabel('Sale Price Quantile')\n",
    "plt.title('Sale Price Quantile vs Distance Quantile')\n",
    "plt.show()\n",
    "\n",
    "# Plot for rarity_rank_quantile and sale_price_quantile\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(merged_df_rareq['rarity_rank_quantile'], merged_df_rareq['sale_price_quantile'],alpha=0.1)\n",
    "plt.xlabel('Rarity Rank Quantile')\n",
    "plt.ylabel('Sale Price Quantile')\n",
    "plt.title('Sale Price Quantile vs Rarity Rank Quantile')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13bebb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "merged_df_rare = merged_df_rare.query(\"sale_price<100\")\n",
    "merged_df_rare['rarity_rank_quantile'] = merged_df_rare.groupby('Collection')['rarity_rank'].transform(lambda x: x.rank(pct=True))\n",
    "merged_df_rare['sale_price_quantile'] = merged_df_rare.groupby('Collection')['sale_price'].transform(lambda x: x.rank(pct=True))\n",
    "merged_df_rare['distance_quantile'] = merged_df_rare.groupby('Collection')['distance'].transform(lambda x: x.rank(pct=True))\n",
    "# Create 100 buckets for 'distance_quantile' and 'rarity_quantile'\n",
    "merged_df_rare['distance_quantile_bucket'] = pd.cut(merged_df_rare['distance_quantile'], bins=np.linspace(0, 1, 51))\n",
    "merged_df_rare['rarity_quantile_bucket'] = pd.cut(merged_df_rare['rarity_rank_quantile'], bins=np.linspace(0, 1, 51))\n",
    "# Calculate the average 'sale_price_quantile' in each 'distance_quantile' bucket\n",
    "average_sale_price_in_distance_bucket = merged_df_rare.groupby('distance_quantile_bucket')['sale_price'].mean()\n",
    "\n",
    "# Calculate the average 'sale_price_quantile' in each 'rarity_quantile' bucket\n",
    "average_sale_price_in_rarity_bucket = merged_df_rare.groupby('rarity_quantile_bucket')['sale_price'].mean()\n",
    "\n",
    "\n",
    "# Calculate the standard deviation of 'sale_price_quantile' in each 'distance_quantile' bucket\n",
    "std_dev_in_distance_bucket = merged_df_rare.groupby('distance_quantile_bucket')['sale_price'].std()\n",
    "\n",
    "# Calculate the standard deviation of 'sale_price_quantile' in each 'rarity_quantile' bucket\n",
    "std_dev_in_rarity_bucket = merged_df_rare.groupby('rarity_quantile_bucket')['sale_price'].std()\n",
    "\n",
    "# Plot for average sale price quantile in each distance quantile bucket with error bars\n",
    "plt.figure(figsize=(10, 6))\n",
    "average_sale_price_in_distance_bucket.plot(kind='line')\n",
    "plt.xlabel('Distance Quantile Bucket')\n",
    "plt.ylabel('Average Sale Price Quantile')\n",
    "plt.title('Average Sale Price Quantile for each Distance Quantile Bucket')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# Plot for average sale price quantile in each rarity quantile bucket with error bars\n",
    "plt.figure(figsize=(10, 6))\n",
    "average_sale_price_in_rarity_bucket.plot(kind='line')\n",
    "plt.xlabel('Rarity Quantile Bucket')\n",
    "plt.ylabel('Average Sale Price Quantile')\n",
    "plt.title('Average Sale Price Quantile for each Rarity Quantile Bucket')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadad76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_rare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d528d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_map\n",
    "with open('../bandwagon/tuples_labels_df.pkl','rb') as f:\n",
    "    error_map = pickle.load(f)\n",
    "error_map[\"Average Error\"] = error_map.groupby(\"Actual\")[\"Error\"].transform(\"mean\")\n",
    "error_map = error_map.groupby('Label').mean().reset_index()\n",
    "error_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "69248ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make corr map in a df with columns label, correlation, p-value\n",
    "corr_df = pd.DataFrame(corr_map).T.reset_index()\n",
    "corr_df.columns = ['Label','correlation','p_value']\n",
    "corr_df\n",
    "#merge with error map on Label\n",
    "merged_df = pd.merge(corr_df,error_map,on='Label')\n",
    "merged_df[\"relative error\"] = merged_df[\"Error\"]/ merged_df[\"Average Error\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693612d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot scatter of error vs correlation\n",
    "merged_df = merged_df.query(\"p_value<0.05\")\n",
    "plt.scatter(merged_df['correlation'],merged_df[\"relative error\"],alpha=0.5)\n",
    "print(spearmanr(merged_df['correlation'],merged_df['relative error']))\n",
    "\n",
    "# plt.xlabel('Correlation')\n",
    "# plt.ylabel('Error')\n",
    "plt.title('Error vs Correlation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92c14e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['residual error'].isinf().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f7b008",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.sort_values('relative error',ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ba7049",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nft_venv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
