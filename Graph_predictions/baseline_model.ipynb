{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import torch\n",
    "from torch_geometric.utils.convert import to_networkx\n",
    "from Openseas_Methods import *\n",
    "from torch_geometric.loader import DataLoader\n",
    "from graph_utils import *\n",
    "from network_utils import *\n",
    "from GraphDataset import GraphDataset\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19934\n",
      "19835\n"
     ]
    }
   ],
   "source": [
    "dataset = GraphDataset('dataset_stor/graph_dataset_3',normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[1706317, 1], edge_index=[2, 6660074], y=[1706317], collection=[1706317], label=[1706317], training_mask=[1706317], test_mask=[1706317], val_mask=[1706317])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.get(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(697)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(list(dataset.get(7).collection))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = to_networkx(dataset.get(0), node_attrs=['collection', 'label', 'training_mask', 'test_mask', 'val_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(graph.nodes[0]['collection'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12381\n",
      "12282\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(1044)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset4 = GraphDataset('dataset_stor/graph_dataset_4',normalize=True)\n",
    "sum(list(dataset4.get(7).collection))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[1989540, 1], edge_index=[2, 8940286], y=[1989540], collection=[1989540], label=[1989540], training_mask=[1989540], test_mask=[1989540], val_mask=[1989540])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset4.get(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 21.2 TiB for an array with shape (1706317, 1706317) and data type int64",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Compute graph-wide metrics once\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# harmonic = nx.harmonic_centrality(graph)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# print('harmonic centrality completed')\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# eigenvector = nx.eigenvector_centrality_numpy(graph)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# print('eigenvector centrality completed')\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m katzcentrality \u001b[38;5;241m=\u001b[39m \u001b[43mnx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkatz_centrality_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcomputed graph-wide metrics\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Function to compute node-specific metrics\u001b[39;00m\n",
      "File \u001b[0;32m/global/scratch/tlundy/NFT_Research/nft_venv/lib/python3.8/site-packages/networkx/utils/decorators.py:766\u001b[0m, in \u001b[0;36margmap.__call__.<locals>.func\u001b[0;34m(_argmap__wrapper, *args, **kwargs)\u001b[0m\n\u001b[1;32m    765\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunc\u001b[39m(\u001b[38;5;241m*\u001b[39margs, __wrapper\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 766\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43margmap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_lazy_compile\u001b[49m\u001b[43m(\u001b[49m\u001b[43m__wrapper\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<class 'networkx.utils.decorators.argmap'> compilation 20:4\u001b[0m, in \u001b[0;36margmap_katz_centrality_numpy_17\u001b[0;34m(G, alpha, beta, normalized, weight)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgzip\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01minspect\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mitertools\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n",
      "File \u001b[0;32m/global/scratch/tlundy/NFT_Research/nft_venv/lib/python3.8/site-packages/networkx/algorithms/centrality/katz.py:326\u001b[0m, in \u001b[0;36mkatz_centrality_numpy\u001b[0;34m(G, alpha, beta, normalized, weight)\u001b[0m\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mAttributeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    324\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m nx\u001b[38;5;241m.\u001b[39mNetworkXError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbeta must be a number\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m--> 326\u001b[0m A \u001b[38;5;241m=\u001b[39m \u001b[43mnx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madjacency_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnodelist\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnodelist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtodense\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m    327\u001b[0m n \u001b[38;5;241m=\u001b[39m A\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    328\u001b[0m centrality \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39msolve(np\u001b[38;5;241m.\u001b[39meye(n, n) \u001b[38;5;241m-\u001b[39m (alpha \u001b[38;5;241m*\u001b[39m A), b)\n",
      "File \u001b[0;32m/global/scratch/tlundy/NFT_Research/nft_venv/lib/python3.8/site-packages/scipy/sparse/_base.py:946\u001b[0m, in \u001b[0;36mspmatrix.todense\u001b[0;34m(self, order, out)\u001b[0m\n\u001b[1;32m    916\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtodense\u001b[39m(\u001b[38;5;28mself\u001b[39m, order\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    917\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;124;03m    Return a dense matrix representation of this matrix.\u001b[39;00m\n\u001b[1;32m    919\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    944\u001b[0m \u001b[38;5;124;03m        `numpy.matrix` object that shares the same memory.\u001b[39;00m\n\u001b[1;32m    945\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 946\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ascontainer(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/global/scratch/tlundy/NFT_Research/nft_venv/lib/python3.8/site-packages/scipy/sparse/_compressed.py:1051\u001b[0m, in \u001b[0;36m_cs_matrix.toarray\u001b[0;34m(self, order, out)\u001b[0m\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m order \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1050\u001b[0m     order \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_swap(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcf\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m-> 1051\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_toarray_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (out\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mc_contiguous \u001b[38;5;129;01mor\u001b[39;00m out\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mf_contiguous):\n\u001b[1;32m   1053\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOutput array must be C or F contiguous\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/global/scratch/tlundy/NFT_Research/nft_venv/lib/python3.8/site-packages/scipy/sparse/_base.py:1298\u001b[0m, in \u001b[0;36mspmatrix._process_toarray_args\u001b[0;34m(self, order, out)\u001b[0m\n\u001b[1;32m   1296\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[1;32m   1297\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1298\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mMemoryError\u001b[0m: Unable to allocate 21.2 TiB for an array with shape (1706317, 1706317) and data type int64"
     ]
    }
   ],
   "source": [
    "# Compute graph-wide metrics once\n",
    "# harmonic = nx.harmonic_centrality(graph)\n",
    "# print('harmonic centrality completed')\n",
    "# eigenvector = nx.eigenvector_centrality_numpy(graph)\n",
    "# print('eigenvector centrality completed')\n",
    "# pagerank = nx.pagerank(graph)\n",
    "# print('computed graph-wide metrics')\n",
    "\n",
    "# Function to compute node-specific metrics\n",
    "def compute_node_specific_metrics(G, node):\n",
    "    metrics = {}\n",
    "\n",
    "    # start_time = time.time()\n",
    "    # Computing each metric and recording the time taken\n",
    "    metrics[\"degree\"] = G.degree(node)\n",
    "    # print('degree', time.time() - start_time)\n",
    "\n",
    "    # start_time = time.time()\n",
    "    metrics[\"clustering_coefficient\"] = nx.clustering(G, node)\n",
    "    # print(\"clustering coefficient\", time.time() - start_time)\n",
    "\n",
    "    # start_time = time.time()\n",
    "    # NOTE: closeness is super slow\n",
    "    # metrics[\"closeness_centrality\"] = nx.closeness_centrality(G, node)\n",
    "    # print(\"closeness centrality\", time.time() - start_time)\n",
    "\n",
    "    # start_time = time.time()\n",
    "    metrics[\"square_clustering\"] = nx.square_clustering(G, node)\n",
    "    # print(\"square_clustering\", time.time() - start_time)\n",
    "\n",
    "    return metrics\n",
    "\n",
    "# Assigning metrics as node attributes for the subset of collection nodes\n",
    "collection_nodes = [node for node in graph if graph.nodes[node]['collection']]\n",
    "for node in tqdm(graph):\n",
    "    if graph.nodes[node]['collection']:\n",
    "        node_metrics = compute_node_specific_metrics(graph, node)\n",
    "    # node_metrics[\"harmonic_centrality\"] = harmonic[node]\n",
    "    node_metrics[\"eigenvector_centrality\"] = eigenvector[node]\n",
    "    # node_metrics[\"pagerank\"] = pagerank[node]\n",
    "    graph.nodes[node]['metrics'] = node_metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "# Assuming each node in the graph has 'label', 'train', 'val', 'test' attributes\n",
    "# Extracting features (metrics) and labels\n",
    "features = []\n",
    "labels = []\n",
    "train_mask = []\n",
    "val_mask = []\n",
    "test_mask = []\n",
    "\n",
    "for node in graph.nodes(data=True):\n",
    "    features.append(list(node[1]['metrics'].values()))\n",
    "    labels.append(node[1]['label'])\n",
    "    train_mask.append(node[1]['train_mask'])\n",
    "    val_mask.append(node[1]['val_mask'])\n",
    "    test_mask.append(node[1]['test_mask'])\n",
    "\n",
    "# Converting lists to PyTorch tensors\n",
    "features = torch.tensor(features, dtype=torch.float32)\n",
    "labels = torch.tensor(labels, dtype=torch.long)\n",
    "train_mask = torch.tensor(train_mask, dtype=torch.bool)\n",
    "val_mask = torch.tensor(val_mask, dtype=torch.bool)\n",
    "test_mask = torch.tensor(test_mask, dtype=torch.bool)\n",
    "\n",
    "# Define a custom dataset\n",
    "class GraphDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "\n",
    "# Creating datasets\n",
    "train_dataset = GraphDataset(features[train_mask], labels[train_mask])\n",
    "val_dataset = GraphDataset(features[val_mask], labels[val_mask])\n",
    "test_dataset = GraphDataset(features[test_mask], labels[test_mask])\n",
    "\n",
    "# DataLoader for batching\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Define the MLP model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "# Initialize the MLP\n",
    "input_size = len(features[0])  # Number of input features\n",
    "hidden_size = 64  # You can tune this\n",
    "num_classes = len(torch.unique(labels))  # Number of unique labels\n",
    "model = MLP(input_size, hidden_size, num_classes)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training the MLP\n",
    "def train(model, data_loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in data_loader:\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "# Validation\n",
    "def validate(model, data_loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data in data_loader:\n",
    "            inputs, labels = data\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 20  # You can tune this\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train(model, train_loader, optimizer, criterion)\n",
    "    val_loss = validate(model, val_loader, criterion)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}')\n",
    "\n",
    "# We can also evaluate the model on the test set after training\n",
    "# However, for brevity, I'm not including that here. You can use the validate function for testing as well.\n",
    "\n",
    "# Note: This is a basic implementation. Depending on your dataset, you might need to adjust the model architecture, hyperparameters, etc.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nft_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
